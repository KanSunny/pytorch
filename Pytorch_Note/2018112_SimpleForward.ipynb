{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.2263 -1.5229 -0.7173\n",
      "-0.6483 -0.3258  0.2499\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 1.2263  0.0000  0.0000\n",
      " 0.0000  0.0000  0.2499\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 1.2263  0.0000  0.0000\n",
      " 0.0000  0.0000  0.2499\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "# >=０, inplace是否用输出覆盖到输入\n",
    "relu = nn.ReLU(inplace=True)\n",
    "input = V(t.randn(2, 3))\n",
    "print(input)\n",
    "output = relu(input)\n",
    "print(output)\n",
    "print(input)# 输入被覆盖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (activation_layer): ReLU ()\n",
      ")\n",
      "Sequential (\n",
      "  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (2): ReLU ()\n",
      ")\n",
      "Sequential (\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu1): ReLU ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#0. 正常的构建方法已经在之前提过了，载forward里面才考虑池化和激活\n",
    "\n",
    "#1. 使用Sequential的方法快速搭建神经网络(__init__部分),还是要写forward的\n",
    "\n",
    "# 通过add_module()添加每一层，并且为每一层增加了一个单独的名字\n",
    "net1 = nn.Sequential()\n",
    "net1.add_module('conv', nn.Conv2d(3, 3, 3))\n",
    "net1.add_module('batchnorm', nn.BatchNorm2d(3))\n",
    "net1.add_module('activation_layer', nn.ReLU())# 需要显示的写出relu等层\n",
    "\n",
    "print(net1)\n",
    "\n",
    "# 这种方法利用torch.nn.Sequential（）容器进行快速搭建，模型的各层被顺序添加到容器中。缺点是每层的编号是默认的阿拉伯数字，不易区分。\n",
    "net2 = nn.Sequential(\n",
    "    nn.Conv2d(3,3,3),\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.ReLU()# 需要显示的写出relu等层\n",
    "    )\n",
    "\n",
    "print(net2)\n",
    "\n",
    "# 是第三种方法的另外一种写法，通过字典的形式添加每一层，并且设置单独的层名称。\n",
    "from collections import OrderedDict\n",
    "net3 = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(3, 3, 3)),\n",
    "    ('bn1', nn.BatchNorm2d(3)),\n",
    "    ('relu1', nn.ReLU())\n",
    "]))\n",
    "\n",
    "print(net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv): Sequential (\n",
      "    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (rel_c1): ReLU ()\n",
      "    (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (rel_c2): ReLU ()\n",
      "    (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (dense): Sequential (\n",
      "    (line1): Linear (400 -> 120)\n",
      "    (rel_d1): ReLU ()\n",
      "    (line2): Linear (120 -> 84)\n",
      "    (rel_d2): ReLU ()\n",
      "    (line3): Linear (84 -> 10)\n",
      "  )\n",
      ")\n",
      "10\n",
      "conv.conv1.weight : torch.Size([6, 1, 5, 5])\n",
      "conv.conv1.bias : torch.Size([6])\n",
      "conv.conv2.weight : torch.Size([16, 6, 5, 5])\n",
      "conv.conv2.bias : torch.Size([16])\n",
      "dense.line1.weight : torch.Size([120, 400])\n",
      "dense.line1.bias : torch.Size([120])\n",
      "dense.line2.weight : torch.Size([84, 120])\n",
      "dense.line2.bias : torch.Size([84])\n",
      "dense.line3.weight : torch.Size([10, 84])\n",
      "dense.line3.bias : torch.Size([10])\n",
      "Variable containing:\n",
      "-0.0008  0.9994  1.9993  2.9986  3.9969  4.9961  5.9948  6.9955  7.9946  8.9960\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 我们使用第三种方法，实现先前的LeNet。通过字典的形式添加每一层，并且设置单独的层名称\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv1\", nn.Conv2d(1, 6, 5)),\n",
    "                    (\"rel_c1\", nn.ReLU()),\n",
    "                    (\"pool1\", nn.MaxPool2d((2,2))),\n",
    "                    (\"conv2\", nn.Conv2d(6, 16, 5)),\n",
    "                    (\"rel_c2\", nn.ReLU()),\n",
    "                    (\"pool2\", nn.MaxPool2d(2))\n",
    "                ]\n",
    "            ))\n",
    "        self.dense = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"line1\", nn.Linear(16*5*5, 120)),\n",
    "                    (\"rel_d1\", nn.ReLU()),\n",
    "                    (\"line2\", nn.Linear(120, 84)),\n",
    "                    (\"rel_d2\", nn.ReLU()),\n",
    "                    (\"line3\", nn.Linear(84, 10))\n",
    "                ]\n",
    "            ))\n",
    "    def forward(self, x):# 前向传播更简单了\n",
    "        conv_out = self.conv(x)\n",
    "        res = conv_out.view(conv_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "#3. 输出参数\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "\n",
    "# 参数，权重矩阵和向上传递的偏差矩阵\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "#４．训练\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)# 设置学习率\n",
    "for i in range(10000):\n",
    "    #4.1 前向传播\n",
    "    input = V(t.randn(1, 1, 32, 32))# 定义输入，这里只用了一个案例输入一个\n",
    "    output = net(input)# 进行一次前向传播\n",
    "\n",
    "    #4.2 计算损失(损失函数)\n",
    "    target = V(t.arange(0, 10))\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    #4.3 反向传播计算损失\n",
    "    net.zero_grad()# 可学习参数清零\n",
    "    loss.backward()# 直接反向传播loss即可\n",
    "    \n",
    "    #4.4 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "#5. 测试\n",
    "input = V(t.randn(1, 1, 32, 32))# 定义输入，这里只用了一个案例输入一个\n",
    "output = net(input)# 进行一次前向传播\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
